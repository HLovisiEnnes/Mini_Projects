{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise from Géron, Aurélien. Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow\n",
    "Train an Encoder– Decoder model that can convert a date string from one format to another (e.g., from “April 22, 2019” to “2019-04-22”)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_addons as tfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us make the training set. Instead of ysing any pre-built function, I will do it by hand to practice strings. Also, even though teh randomization method here will not be the most computational efficient, I decided for it to be able to have more control."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_builder(number_of_iterations):\n",
    "    output = []\n",
    "    \n",
    "    day_list = list(range(1, 29))\n",
    "    month_list = [\"January\", \"February\", \"March\", \"April\", \"May\", \"June\", \n",
    "                  \"July\", \"August\", \"September\", \"October\", \"November\", \"December\"]\n",
    "    month_number_list = list(range(12))\n",
    "    list_year = list(range(1809, 2021)) \n",
    "    \n",
    "    for i in range(number_of_iterations):\n",
    "        day = random.choice(day_list)\n",
    "        month_number = random.choice(month_number_list)\n",
    "        month = month_list[month_number]\n",
    "        year = random.choice(list_year)\n",
    "        \n",
    "        long_date = str(month)+\" \"+str(day)+\", \"+str(year)\n",
    "        short_date = str(year)+\"-\"+str(month_number+1)+\"-\"+str(day)\n",
    "        \n",
    "        time_step = [long_date, short_date]\n",
    "        output.append(time_step)\n",
    "    \n",
    "    return np.array(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "212"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(range(1809, 2021)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['April 9, 1960', '1960-4-9'],\n",
       "       ['January 14, 1940', '1940-1-14'],\n",
       "       ['June 9, 1957', '1957-6-9'],\n",
       "       ['May 18, 1828', '1828-5-18'],\n",
       "       ['February 24, 1955', '1955-2-24'],\n",
       "       ['April 7, 1989', '1989-4-7'],\n",
       "       ['May 3, 1809', '1809-5-3'],\n",
       "       ['May 4, 1831', '1831-5-4'],\n",
       "       ['November 2, 1867', '1867-11-2'],\n",
       "       ['September 20, 1830', '1830-9-20']], dtype='<U18')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = dataset_builder(10)\n",
    "test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, all working. Time to make a few data set sizes so we can train with all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_500, dataset_5000, dataset_50000 = dataset_builder(500), dataset_builder(5000),dataset_builder(50000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us make the preprocess the data. First, let us make the dictionary to numberize the data. We start by making a list of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = [str(year) for year in list(range(1809, 2021))]\n",
    "for month in [\"January\", \"February\", \"March\", \"April\", \"May\", \"June\", \n",
    "                  \"July\", \"August\", \"September\", \"October\", \"November\", \"December\"]:\n",
    "    input_data.append(month)\n",
    "for day in list(range(1, 29)):\n",
    "    input_data.append(str(day)+\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'28,'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data = [str(year) for year in list(range(1809, 2021))]\n",
    "for day in list(range(1, 29)): #we will not do string for day to avoid double-assigning numbers\n",
    "    output_data.append(str(day))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(252, 240)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_data), len(output_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we create the dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dictionary = dict([(y,x+1) for x,y in enumerate(sorted(set(input_data)))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dictionary = dict([(y,x+1) for x,y in enumerate(sorted(set(output_data)))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us just try the dictionaries out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['April', '9,', '1960']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[0][0].split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[241, 240, 163]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_input = test_dataset[0][0].split()\n",
    "[input_dictionary[x] for x in test_input]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(241, 4, 35)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dictionary['April'],input_dictionary['12,'],input_dictionary['1833']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1960', '4', '9']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[0][1].split(\"-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[163, 235, 240]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_output = test_dataset[0][1].split(\"-\")\n",
    "[output_dictionary[x] for x in test_output]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35, 235, 4)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dictionary['1833'],output_dictionary['4'],output_dictionary['12']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good, let us make the sets, already numberized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data, percentage_train, percentage_valid): #inputs the dataset, numnber of steps, \n",
    "    #the percentage of data that will be used, for training, and the percentage of data used for validation, both in decimal form\n",
    "    #percentage for test is given by subtracting both\n",
    "    #outputs the six X_train, y_train, X_valid,...\n",
    "    X_list, y_list = data[:,0], data[:,1]\n",
    "    X = []\n",
    "    y = []\n",
    "    X_decoder = []\n",
    "    \n",
    "    decoder_value = len(output_dictionary) + 1 #create the first entry to be able to shift the decoder inputs one step\n",
    "    \n",
    "    for i in range(len(X_list)):\n",
    "        X_i = X_list[i].split()\n",
    "        y_i = y_list[i].split(\"-\")\n",
    "        \n",
    "        X_i_numberized = [input_dictionary[x] for x in X_i] #numberize the data\n",
    "        y_i_numberized = [output_dictionary[x] for x in y_i]\n",
    "        \n",
    "        X_decoder_i = [decoder_value]\n",
    "        X_decoder_i = X_decoder_i + y_i_numberized[:-1] #creates a list so that we shift every entry by one\n",
    "        \n",
    "        X.append(np.array(X_i_numberized))\n",
    "        y.append(np.array(y_i_numberized))\n",
    "        X_decoder.append(np.array(X_decoder_i))\n",
    "    \n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    X_decoder = np.array(X_decoder)\n",
    "    \n",
    "    \n",
    "    train_size = int(len(data)*percentage_train) #eventhough we might loose one or two data points by using int, given the \n",
    "    #dataset sizes, they are not super important\n",
    "    valid_size = int(len(data)*percentage_valid)\n",
    "    test_size = int(len(data)-train_size-valid_size)\n",
    "    \n",
    "    X_train, X_valid, X_test = X[:train_size], X[train_size:train_size+valid_size], X[train_size+valid_size:]\n",
    "    y_train, y_valid, y_test = y[:train_size], y[train_size:train_size+valid_size], y[train_size+valid_size:]\n",
    "    X_train_decoder, X_valid_decoder, X_test_decoder = X_decoder[:train_size], X_decoder[train_size:train_size+valid_size], X_decoder[train_size+valid_size:]\n",
    "    \n",
    "    return  X_train, X_valid, X_test, y_train, y_valid, y_test, X_train_decoder, X_valid_decoder, X_test_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[241, 240, 163],\n",
       "        [245,   6, 143],\n",
       "        [247, 240, 160],\n",
       "        [249,  10,  30],\n",
       "        [244, 229, 158],\n",
       "        [241, 238, 192],\n",
       "        [249, 234,  11],\n",
       "        [249, 235,  33]]),\n",
       " array([[250, 203,  69]]),\n",
       " array([[252, 204,  32]]),\n",
       " array([[163, 235, 240],\n",
       "        [143,   1,   6],\n",
       "        [160, 237, 240],\n",
       "        [ 30, 236,  10],\n",
       "        [158, 203, 229],\n",
       "        [192, 235, 238],\n",
       "        [ 11, 236, 234],\n",
       "        [ 33, 236, 235]]),\n",
       " array([[ 69,   3, 203]]),\n",
       " array([[ 32, 240, 204]]),\n",
       " array([[241, 163, 235],\n",
       "        [241, 143,   1],\n",
       "        [241, 160, 237],\n",
       "        [241,  30, 236],\n",
       "        [241, 158, 203],\n",
       "        [241, 192, 235],\n",
       "        [241,  11, 236],\n",
       "        [241,  33, 236]]),\n",
       " array([[241,  69,   3]]),\n",
       " array([[241,  32, 240]]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(test_dataset, 0.8, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 20\n",
    "vocab_size_input = len(input_data)\n",
    "vocab_size_output = len(output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "units = 252\n",
    "\n",
    "encoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
    "decoder_inputs = keras.layers.Input(shape=[None], dtype=np.int32)\n",
    "sequence_lengths = keras.layers.Input(shape=[], dtype=np.int32)\n",
    "\n",
    "encoder_embeddings = keras.layers.Embedding(vocab_size_input + 1, embed_size)(encoder_inputs)\n",
    "\n",
    "decoder_embedding_layer = keras.layers.Embedding(vocab_size_input + 2, embed_size)\n",
    "decoder_embeddings = decoder_embedding_layer(decoder_inputs)\n",
    "\n",
    "encoder = keras.layers.LSTM(units, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_embeddings)\n",
    "encoder_state = [state_h, state_c]\n",
    "\n",
    "sampler = tfa.seq2seq.sampler.TrainingSampler()\n",
    "\n",
    "decoder_cell = keras.layers.LSTMCell(units)\n",
    "output_layer = keras.layers.Dense(vocab_size_output + 1)\n",
    "\n",
    "decoder = tfa.seq2seq.basic_decoder.BasicDecoder(decoder_cell,sampler,output_layer=output_layer)\n",
    "final_outputs, final_state, final_sequence_lengths = decoder(decoder_embeddings,initial_state=encoder_state)\n",
    "Y_proba = keras.layers.Activation(\"softmax\")(final_outputs.rnn_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us run the model for each set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, X_test, y_train, y_valid, y_test, X_train_decoder, X_valid_decoder, X_test_decoder = preprocess(dataset_500, 0.8,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400, 3)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_500 = keras.models.Model(inputs=[encoder_inputs, decoder_inputs],outputs=[Y_proba])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Nadam()\n",
    "model_500.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 400 samples, validate on 50 samples\n",
      "Epoch 1/20\n",
      "400/400 [==============================] - 39s 97ms/sample - loss: 5.4519 - accuracy: 0.0525 - val_loss: 5.2116 - val_accuracy: 0.0600\n",
      "Epoch 2/20\n",
      "400/400 [==============================] - 3s 8ms/sample - loss: 4.4720 - accuracy: 0.0417 - val_loss: 4.1833 - val_accuracy: 0.0333\n",
      "Epoch 3/20\n",
      "400/400 [==============================] - 2s 5ms/sample - loss: 4.1256 - accuracy: 0.0408 - val_loss: 4.1434 - val_accuracy: 0.0333\n",
      "Epoch 4/20\n",
      "400/400 [==============================] - 2s 5ms/sample - loss: 4.0631 - accuracy: 0.0367 - val_loss: 4.1292 - val_accuracy: 0.0333\n",
      "Epoch 5/20\n",
      "400/400 [==============================] - 2s 5ms/sample - loss: 4.0085 - accuracy: 0.0517 - val_loss: 4.1103 - val_accuracy: 0.0333\n",
      "Epoch 6/20\n",
      "400/400 [==============================] - 2s 5ms/sample - loss: 3.9518 - accuracy: 0.0500 - val_loss: 4.0791 - val_accuracy: 0.0333\n",
      "Epoch 7/20\n",
      "400/400 [==============================] - 2s 4ms/sample - loss: 3.8898 - accuracy: 0.0467 - val_loss: 4.0413 - val_accuracy: 0.0267\n",
      "Epoch 8/20\n",
      "400/400 [==============================] - 2s 5ms/sample - loss: 3.8189 - accuracy: 0.0567 - val_loss: 4.0198 - val_accuracy: 0.0267\n",
      "Epoch 9/20\n",
      "400/400 [==============================] - 2s 5ms/sample - loss: 3.7333 - accuracy: 0.0683 - val_loss: 3.9385 - val_accuracy: 0.0733\n",
      "Epoch 10/20\n",
      "400/400 [==============================] - 2s 5ms/sample - loss: 3.6582 - accuracy: 0.0658 - val_loss: 3.8905 - val_accuracy: 0.1000\n",
      "Epoch 11/20\n",
      "400/400 [==============================] - 2s 4ms/sample - loss: 3.5805 - accuracy: 0.0983 - val_loss: 3.8656 - val_accuracy: 0.0733\n",
      "Epoch 12/20\n",
      "400/400 [==============================] - 2s 5ms/sample - loss: 3.4983 - accuracy: 0.1175 - val_loss: 3.8189 - val_accuracy: 0.1133\n",
      "Epoch 13/20\n",
      "400/400 [==============================] - 2s 5ms/sample - loss: 3.4194 - accuracy: 0.1517 - val_loss: 3.7776 - val_accuracy: 0.1200\n",
      "Epoch 14/20\n",
      "400/400 [==============================] - 2s 5ms/sample - loss: 3.2694 - accuracy: 0.1917 - val_loss: 3.6760 - val_accuracy: 0.1600\n",
      "Epoch 15/20\n",
      "400/400 [==============================] - 2s 5ms/sample - loss: 3.1611 - accuracy: 0.2475 - val_loss: 3.5763 - val_accuracy: 0.2467\n",
      "Epoch 16/20\n",
      "400/400 [==============================] - 2s 4ms/sample - loss: 2.9850 - accuracy: 0.2867 - val_loss: 3.5334 - val_accuracy: 0.2467\n",
      "Epoch 17/20\n",
      "400/400 [==============================] - 2s 5ms/sample - loss: 2.8628 - accuracy: 0.3292 - val_loss: 3.4295 - val_accuracy: 0.2867\n",
      "Epoch 18/20\n",
      "400/400 [==============================] - 2s 4ms/sample - loss: 2.7419 - accuracy: 0.3492 - val_loss: 3.5594 - val_accuracy: 0.2933\n",
      "Epoch 19/20\n",
      "400/400 [==============================] - 2s 5ms/sample - loss: 2.6356 - accuracy: 0.3500 - val_loss: 3.3565 - val_accuracy: 0.3000\n",
      "Epoch 20/20\n",
      "400/400 [==============================] - 2s 4ms/sample - loss: 2.5278 - accuracy: 0.3675 - val_loss: 3.3477 - val_accuracy: 0.3133\n"
     ]
    }
   ],
   "source": [
    "history = model_500.fit([X_train, X_train_decoder], y_train, epochs=20, validation_data=([X_valid, X_valid_decoder], y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50/50 [==============================] - 0s 9ms/sample - loss: 3.3527 - accuracy: 0.3533\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[3.352668743133545, 0.35333332]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_500.evaluate([X_test, X_test_decoder], y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, X_test, y_train, y_valid, y_test, X_train_decoder, X_valid_decoder, X_test_decoder = preprocess(dataset_5000, 0.8,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_5000 = keras.models.Model(inputs=[encoder_inputs, decoder_inputs],outputs=[Y_proba])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Nadam()\n",
    "model_5000.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4000 samples, validate on 500 samples\n",
      "Epoch 1/20\n",
      "4000/4000 [==============================] - 41s 10ms/sample - loss: 2.8877 - accuracy: 0.3708 - val_loss: 2.6044 - val_accuracy: 0.4093\n",
      "Epoch 2/20\n",
      "4000/4000 [==============================] - 12s 3ms/sample - loss: 2.4244 - accuracy: 0.4810 - val_loss: 2.4512 - val_accuracy: 0.4753\n",
      "Epoch 3/20\n",
      "4000/4000 [==============================] - 12s 3ms/sample - loss: 2.1049 - accuracy: 0.5970 - val_loss: 2.0372 - val_accuracy: 0.6373\n",
      "Epoch 4/20\n",
      "4000/4000 [==============================] - 12s 3ms/sample - loss: 1.8842 - accuracy: 0.6566 - val_loss: 1.9093 - val_accuracy: 0.6640\n",
      "Epoch 5/20\n",
      "4000/4000 [==============================] - 12s 3ms/sample - loss: 1.7650 - accuracy: 0.6700 - val_loss: 1.8362 - val_accuracy: 0.6673\n",
      "Epoch 6/20\n",
      "4000/4000 [==============================] - 12s 3ms/sample - loss: 1.6492 - accuracy: 0.6783 - val_loss: 1.7186 - val_accuracy: 0.6747\n",
      "Epoch 7/20\n",
      "4000/4000 [==============================] - 12s 3ms/sample - loss: 1.5153 - accuracy: 0.6838 - val_loss: 1.5620 - val_accuracy: 0.6833\n",
      "Epoch 8/20\n",
      "4000/4000 [==============================] - 12s 3ms/sample - loss: 1.2971 - accuracy: 0.7169 - val_loss: 1.3640 - val_accuracy: 0.6967\n",
      "Epoch 9/20\n",
      "4000/4000 [==============================] - 12s 3ms/sample - loss: 1.0953 - accuracy: 0.7539 - val_loss: 1.0968 - val_accuracy: 0.7500\n",
      "Epoch 10/20\n",
      "4000/4000 [==============================] - 12s 3ms/sample - loss: 0.7713 - accuracy: 0.8528 - val_loss: 0.7681 - val_accuracy: 0.8493\n",
      "Epoch 11/20\n",
      "4000/4000 [==============================] - 12s 3ms/sample - loss: 0.4944 - accuracy: 0.9426 - val_loss: 0.5084 - val_accuracy: 0.9273\n",
      "Epoch 12/20\n",
      "4000/4000 [==============================] - 12s 3ms/sample - loss: 0.3367 - accuracy: 0.9735 - val_loss: 0.3992 - val_accuracy: 0.9547\n",
      "Epoch 13/20\n",
      "4000/4000 [==============================] - 12s 3ms/sample - loss: 0.1889 - accuracy: 0.9959 - val_loss: 0.2089 - val_accuracy: 0.9933\n",
      "Epoch 14/20\n",
      "4000/4000 [==============================] - 12s 3ms/sample - loss: 0.1058 - accuracy: 0.9998 - val_loss: 0.1388 - val_accuracy: 0.9980\n",
      "Epoch 15/20\n",
      "4000/4000 [==============================] - 13s 3ms/sample - loss: 0.0691 - accuracy: 1.0000 - val_loss: 0.0997 - val_accuracy: 0.9987\n",
      "Epoch 16/20\n",
      "4000/4000 [==============================] - 12s 3ms/sample - loss: 0.0488 - accuracy: 1.0000 - val_loss: 0.0744 - val_accuracy: 0.9993\n",
      "Epoch 17/20\n",
      "4000/4000 [==============================] - 12s 3ms/sample - loss: 0.0365 - accuracy: 1.0000 - val_loss: 0.0589 - val_accuracy: 0.9993\n",
      "Epoch 18/20\n",
      "4000/4000 [==============================] - 12s 3ms/sample - loss: 0.0284 - accuracy: 1.0000 - val_loss: 0.0473 - val_accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "4000/4000 [==============================] - 12s 3ms/sample - loss: 0.0228 - accuracy: 1.0000 - val_loss: 0.0392 - val_accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "4000/4000 [==============================] - 12s 3ms/sample - loss: 0.0187 - accuracy: 1.0000 - val_loss: 0.0332 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "history = model_5000.fit([X_train, X_train_decoder], y_train, epochs=20, validation_data=([X_valid, X_valid_decoder], y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500/500 [==============================] - 1s 2ms/sample - loss: 0.0363 - accuracy: 0.9993\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.036339461877942084, 0.9993333]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_5000.evaluate([X_test, X_test_decoder], y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us make a function that makes predictions. First we must reverse the dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_reverse = dict((v, k) for k, v in output_dictionary.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(input_string):\n",
    "    input_string = input_string.split()\n",
    "    input_numbers = [input_dictionary[x] for x in input_string]\n",
    "    input_numbers = np.array(input_numbers)\n",
    "    input_numbers = input_numbers.reshape(1,3)   \n",
    "    \n",
    "    prediction = list(np.argmax(model_5000.predict([test_input_numbers,test_input_numbers])[0], axis=1))\n",
    "    \n",
    "    \n",
    "    prediction = [dictionary_reverse[x] for x in prediction]\n",
    "    \n",
    "    return prediction[0]+\"-\"+prediction[1]+\"-\"+prediction[2]    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On my birthday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_input_numbers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-d186903055a6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprediction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"February 4, 1998\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-37-7187fe9fef57>\u001b[0m in \u001b[0;36mprediction\u001b[1;34m(input_string)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0minput_numbers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_numbers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_5000\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest_input_numbers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_input_numbers\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_input_numbers' is not defined"
     ]
    }
   ],
   "source": [
    "prediction(\"February 4, 1998\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
